{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50aae090-7a18-41f6-a817-dbf025c26de0",
   "metadata": {},
   "source": [
    "# Brainlife analyses using Jupyter Notebooks and Python3\n",
    "\n",
    "This example notebook will guide the user through grabbing data for their project from the secondary warehouse, compiling data across the entire project, and analyzing and visualizing the compiled results on brainlife.io via the 'Analysis' tab. This example is written for python3, and uses one of the Python3 notebook types.\n",
    "\n",
    "Within this notebook, I will guide the user through analyses comprising derivatives generated from three main datatypes available on brainlife.io, specifically neuro/anat/t1w & t2w. Specifically, I will guide the user through:\n",
    "    \n",
    "    1. Analyze functional connectivity properties derived from functional network matricies generated from fMRI using the Yeo 17 atlas.\n",
    "    \n",
    "The functions provided in this notebook were designed for performing the many experiments provided in Hayashi & Caron et al (in prep), but should be a good starting point for you to get started on analyzing your project on brainlife.io!\n",
    "\n",
    "### Important Note regarding assumptions of data structures, both inputs and outputs\n",
    "\n",
    "Before we get started, it is important to note at the begging that the code in this notebook was designed mostly with a stereotypical data structure format in mind. For many of the datatypes, specifically the csv datatypes, follow the tidy format [https://www.jstatsoft.org/article/view/v059i10], where \"each column is a variable, every row is an observation, and every cell is a singular value\"[https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html]. There will be a few common columns, specifically 'subjectID' denoting the brainlife subject ID, 'sessionID' denoting the session ID on brainlife (will be 1 if there is no session ID data available), 'structureID' denoting the tissue structure of interest (ex. white matter tract, cortical parcel, eeg node), and 'nodeID' denoting the sample number of the structure. 'nodeID' is included across many .csv-based datatypes to conform with profilometry data, which has multiple samples or nodes per structure. For single measures of a given structure, the nodeID for all structures will be 1. \n",
    "\n",
    "However, this is not the case for network adjacency matrices. Specifically with the network.json.gz files that come within the generic/network datatype, the resulting compiled data structure will be compiled as a json structure with a numpy array of the matrix for each subject and session. \n",
    "\n",
    "I will provide examples of these resulting structures later on in the notebook.\n",
    "\n",
    "I bring this up now to note that if you are not a fan of the style of the resulting data structures, or your data doesn't confirm to this style, you are free to alter this code to your deepest desires! I've attempted to do my best to comment and explain everything as much as possible for each section, function, and analysis. If there are any points where something is not working or you are not sure what the code is doing, feel free to email me at [bacaron245@gmail.com!]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c9f20-fac0-494c-af2d-812ef6a43cec",
   "metadata": {},
   "source": [
    "# With that, let's get started!\n",
    "\n",
    "## Install packages\n",
    "The first things we need to do are to install some software packages using python's pip package manager, and then to load all of the necessary modules into our notebook.\n",
    "\n",
    "Specifically, we will install the python-import of the brain connectivity toolbox (BCT; ) known as bctpy [], jgf to load json packages (for our network matrices), and nilearn to make some pretty network plots!\n",
    "\n",
    "We will also be loading some other packages, including matplotlib, seaborn, and nilearn for plotting, numpy for numerical calculations and array manipulations, itertools and scipy for statistical testing, pandas, json, and jgf for loading various datat strucutres, and scikit-learn's mean_squared_error function to compute root-mean-square-error for some of the final analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3768115-29f8-44e0-98c7-7a08bf1d78c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bctpy in /home/brad/.local/lib/python3.8/site-packages (0.5.2)\n",
      "Requirement already satisfied: jgf in /home/brad/.local/lib/python3.8/site-packages (0.2.2)\n",
      "Requirement already satisfied: nilearn in /home/brad/.local/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy in /home/brad/.local/lib/python3.8/site-packages (from bctpy) (1.21.5)\n",
      "Requirement already satisfied: scipy in /home/brad/.local/lib/python3.8/site-packages (from bctpy) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/brad/.local/lib/python3.8/site-packages (from nilearn) (1.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in /home/brad/.local/lib/python3.8/site-packages (from nilearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.12 in /home/brad/.local/lib/python3.8/site-packages (from nilearn) (1.1.0)\n",
      "Requirement already satisfied: requests>=2 in /usr/lib/python3/dist-packages (from nilearn) (2.22.0)\n",
      "Requirement already satisfied: nibabel>=2.5 in /home/brad/.local/lib/python3.8/site-packages (from nilearn) (3.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/brad/.local/lib/python3.8/site-packages (from pandas>=0.24.0->nilearn) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/brad/.local/lib/python3.8/site-packages (from pandas>=0.24.0->nilearn) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/brad/.local/lib/python3.8/site-packages (from scikit-learn>=0.21->nilearn) (3.1.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/brad/.local/lib/python3.8/site-packages (from nibabel>=2.5->nilearn) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.0->nilearn) (1.14.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=14.3->nibabel>=2.5->nilearn) (2.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bctpy jgf nilearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471eb1d8-0465-4467-913d-194b6f462e02",
   "metadata": {},
   "source": [
    "## Importing packages\n",
    "The first things we need to do is to load python packages in order to perform actions on the processed data, including matplotlib and seaborn for plotting, numpy for numerical calculations and array manipulations, itertools and scipy for statistical testing, pandas, and scikit-learn's mean_squared_error function to compute root-mean-square-error for some of the final analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e580b3-9b96-45bc-88a1-c115f35bf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,glob\n",
    "import requests\n",
    "from matplotlib import colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import bct\n",
    "import jgf\n",
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b181361-dbbe-4d7a-980e-a094d6c345d5",
   "metadata": {},
   "source": [
    "## Let's define and load some useful functions!\n",
    "\n",
    "This next section loads many functions for loading, compiling, analyzing, and visualizing some common datatypes on brainlife.io. To make it easier to load, I've provided all the functions used throughout the rest of the notebook in a single cell. However, for each function, I've provided comments that attempt to explain what the function does and what the inputs and outputs are. I've sectioned off the code into multiple sections: 1) data loading, compilation, and storage, 2) data analysis, manipulation and statistical computations, and 3) visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db02474-819c-43be-98d3-dde2c0906f79",
   "metadata": {},
   "source": [
    "### Data loading, compilation, and storage\n",
    "The first section of functions deals with collecting data from secondary warehouse and compiling the data into a useful structures for further analysis. The main function the user will call from this section is the wrapper 'collectData', which will identify the appropriate data from the secondary warehouse based on user input and compile that data into a structure the user can use for analysis. This function makes a couple of important assumptions. Specifically:\n",
    "\n",
    "    1. If there are duplicate datatypes that meet all of the specifications and filters, this function will only collect the data that was most recent to finish\n",
    "    \n",
    "    2. If the data can be formatted to a csv-file, it will be formatted to a csv-file. The only exception is network adjacency matrices.\n",
    "    \n",
    "    3. Users can control what data is grabbed and compiled by being specific with datatype_tags and tags.\n",
    "    \n",
    "The remaining functions perform other useful tasks, such as adding subjectID and sessionID data if missing, checking for duplicate entries of the same datatype (assumption 1), and append and compile data into the output data structure.\n",
    "\n",
    "As previously mentioned, the resulting csv-file structures will be in the tidy format, and network adjacency matricies will be stored as a json-structure with the matricies stored as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9723006-39af-403e-9df4-fc9c13cf50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will add a subjectID and sessionID column to the output data\n",
    "def addSubjectsSessions(subject,session,path,data):\n",
    "    \n",
    "    if 'subjectID' not in data.keys():\n",
    "        data['subjectID'] = [ str(subject) for f in range(len(data)) ]\n",
    "    \n",
    "    if 'sessionID' not in data.keys():\n",
    "        data['sessionID'] = [ str(session) for f in range(len(data)) ]\n",
    "        \n",
    "    return data\n",
    "\n",
    "## this function is useful for identifying duplicate datatypes. if it finds one, it will update the data with the latest finishing dataset.\n",
    "def checkForDuplicates(obj,finish_dates,subjects,sessions,paths):\n",
    "    \n",
    "    # first checks if there is a session id available in the keys of the object. if finds one, then checks if the subject and session ID \n",
    "    # were already looped over. if so, will delete position in list and update with appropriate path. if it doesn't find a session ID, it\n",
    "    # just attempts to find if the subject has already been looped over\n",
    "    if 'session' in obj['output']['meta'].keys():\n",
    "        if (obj['output']['meta']['subject'] in subjects) and (obj['output']['meta']['session'] in sessions):\n",
    "            index = np.where(np.logical_and(subjects == obj['output']['meta']['subject'],sessions == obj['output']['meta']['session']))\n",
    "            if finish_dates[index] <= obj[\"finish_date\"]:\n",
    "                subjects = np.delete(subjects,index)\n",
    "                paths = np.delete(paths,index)\n",
    "                sessions = np.delete(sessions,index)\n",
    "                finish_dates = np.delete(finish_dates,index)\n",
    "    else:\n",
    "        if (obj['output']['meta']['subject'] in subjects):\n",
    "            index = np.where(subjects == obj['output']['meta']['subject'])\n",
    "            if finish_dates[index] <= obj[\"finish_date\"]:\n",
    "                subjects = np.delete(subjects,index)\n",
    "                paths = np.delete(paths,index)\n",
    "                sessions = np.delete(sessions,index)\n",
    "                finish_dates = np.delete(finish_dates,index)\n",
    "\n",
    "    return finish_dates, subjects, sessions, paths\n",
    "\n",
    "## this function calles checkForDuplicates and attempts to find duplicates. then uses that output, sets a dumby sessionID if not present,\n",
    "## and appends the object data\n",
    "def appendData(subjects,sessions,paths,finish_dates,obj,filename):\n",
    "        \n",
    "    # check for duplicates. if so, remove\n",
    "    finish_dates, subjects, sessions, paths = checkForDuplicates(obj,finish_dates,subjects,sessions,paths)\n",
    "\n",
    "    # append data to appropriate lists\n",
    "    subjects = np.append(subjects,obj['output']['meta']['subject'])\n",
    "    if 'session' in obj['output']['meta'].keys():\n",
    "        sessions = np.append(sessions,obj['output']['meta']['session'])\n",
    "    else:\n",
    "        sessions = np.append(sessions,'1')\n",
    "    paths = np.append(paths,\"input/\"+obj[\"path\"]+\"/\"+filename)\n",
    "    finish_dates = np.append(finish_dates,obj['finish_date'])\n",
    "    \n",
    "    return finish_dates, subjects, sessions, paths\n",
    "\n",
    "## this function will call addSubjectsSessions to add the appropriate columns and will append the object data to a study-wide dataframe\n",
    "def compileData(paths,subjects,sessions,data):\n",
    "    # loops through all paths\n",
    "    for i in range(len(paths)):\n",
    "        # if network, load json. if not, load csv\n",
    "        if '.json.gz' in paths[i]:\n",
    "            tmpdata = pd.read_json(paths[i],orient='index').reset_index(drop=True)\n",
    "            tmpdata = addSubjectsSessions(subjects[i],sessions[i],paths[i],tmpdata)\n",
    "        else:\n",
    "            tmpdata = pd.read_csv(paths[i])\n",
    "            tmpdata = addSubjectsSessions(subjects[i],sessions[i],paths[i],tmpdata)\n",
    "\n",
    "        data = data.append(tmpdata,ignore_index=True)\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def compileNetworkAdjacencyMatrices(paths,subjects,sessions,data):\n",
    "    \n",
    "    # loop through paths and append adjacency matrix to dictionary\n",
    "    for i in range(len(paths)):\n",
    "        data[subjects[i]+'_sess'+sessions[i]] = jgf.conmat.load(paths[i],compressed=True)[0]\n",
    "\n",
    "    return data\n",
    "\n",
    "## this function is the wrapper function that calls all the prevouis functions to generate a dataframe for the entire project of the appropriate datatype\n",
    "def collectData(datatype,datatype_tags,tags,filename,outPath,net_adj):\n",
    "\n",
    "    # grab path and data objects\n",
    "    objects = requests.get('https://brainlife.io/api/warehouse/secondary/list/%s'%os.environ['PROJECT_ID']).json()\n",
    "    \n",
    "    # subjects and paths\n",
    "    subjects = []\n",
    "    sessions = []\n",
    "    paths = []\n",
    "    finish_dates = []\n",
    "    \n",
    "    # set up output\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # loop through objects and find appropriate objects based on datatype, datatype_tags, and tags. can include drop tags ('!'). this logic could probably be simplified\n",
    "    for obj in objects:\n",
    "        if obj['datatype']['name'] == datatype:\n",
    "            # if datatype_tags is set, identify data using this info. if not, just use tag data. if no tags either, just append if meets datatype criteria. will check for filter with a not tag (!)\n",
    "            if datatype_tags:\n",
    "                # if the input datatype_tags are included in the object's datatype_tags, look for appropriate tags. if no tags, just append\n",
    "                if set(datatype_tags).issubset(obj['output']['datatype_tags']):\n",
    "                    # if tags is set, identify the data using this info\n",
    "                    if tags:\n",
    "                        # if input tags are included in object's tags, append. check if user wants to filter with a not tag (!)\n",
    "                        if set(tags).issubset(obj['output']['tags']):\n",
    "                            finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                        elif '!' in str(tags):\n",
    "                            tag = [ f for f in tags if '!' in str(f) ]\n",
    "                            tag_drop = [ f for f in tags if f not in tag ]\n",
    "                            if not set([ f.replace('!','') for f in tag]).issubset(obj['output']['tags']):\n",
    "                                if set(tag_drop).issubset(obj['output']['tags']):\n",
    "                                    finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                    else:\n",
    "                        finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                elif '!' in str(datatype_tags): # if a not filter was detected in the tag, clean tags to remove !, skip any objects that have those tags, append those that do not have those tags\n",
    "                    datatype_tag = [ f for f in datatype_tags if '!' in str(f) ]\n",
    "                    datatype_tag_drop = [ f for f in datatype_tags if f not in datatype_tag ]\n",
    "                    if not set([ f.replace('!','') for f in datatype_tag]).issubset(obj['output']['datatype_tags']):\n",
    "                        if tags:\n",
    "                            # if input tags are included in object's tags, append. check if user wants to filter with a not tag (!)\n",
    "                            if set(tags).issubset(obj['output']['tags']):\n",
    "                                finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                            elif '!' in str(tags):\n",
    "                                tag = [ f for f in tags if '!' in str(f) ]\n",
    "                                tag_drop = [ f for f in tags if f not in tag ]\n",
    "                                if not set([ f.replace('!','') for f in tag]).issubset(obj['output']['tags']):\n",
    "                                    if set(tag_drop).issubset(obj['output']['tags']):\n",
    "                                        finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                        else:\n",
    "                            finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "            else:\n",
    "                if tags:\n",
    "                # if input tags are included in object's tags, append. check if user wants to filter with a not tag (!)\n",
    "                    if set(tags).issubset(obj['output']['tags']):\n",
    "                        finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                    elif '!' in str(tags):\n",
    "                        tag = [ f for f in tags if '!' in str(f) ]\n",
    "                        tag_drop = [ f for f in tags if f not in tag ]\n",
    "                        if not set([ f.replace('!','') for f in tag]).issubset(obj['output']['tags']):\n",
    "                            if set(tag_drop).issubset(obj['output']['tags']):\n",
    "                                finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                else:\n",
    "                    finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "\n",
    "    # shuffle data so subjects are in order\n",
    "    paths = [z for _,_,z in sorted(zip(subjects,sessions,paths))]\n",
    "    subjects = [x for x,_,_ in sorted(zip(subjects,sessions,paths))]\n",
    "    sessions = [y for _,y,_ in sorted(zip(subjects,sessions,paths))]\n",
    "    \n",
    "    # compile data\n",
    "    if net_adj:\n",
    "        data = {}\n",
    "        data = compileNetworkAdjacencyMatrices(paths,subjects,sessions,data)\n",
    "        np.save(outPath,data)\n",
    "    else:\n",
    "        data = compileData(paths,subjects,sessions,data)\n",
    "    \n",
    "        # output data structure for records and any further analyses\n",
    "        # subjects.csv\n",
    "        data.to_csv(outPath,index=False)\n",
    "    \n",
    "    return data\n",
    "\n",
    "### subjects dataframe generation\n",
    "## this function will make a dataframe from a list of subjects and groups. will also add a color column for easy plotting\n",
    "def collectSubjectData(topPath,dataPath,groups,subjects,colors):\n",
    "\n",
    "    # set up variables\n",
    "    data_columns = ['subjectID','classID','colors']\n",
    "    data =  pd.DataFrame([],columns=data_columns)\n",
    "\n",
    "    # populate structure\n",
    "    data['subjectID'] = [ f for g in groups for f in subjects[g] ]\n",
    "    data['classID'] = [ g for g in groups for f in range(len(subjects[g]))]\n",
    "    data['colors'] = [ colors[c] for c in colors for f in subjects[c]]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'subjects.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "## this will create a subject-specific color for each subject in the subjects dataframe\n",
    "def createColorDictionary(data,measure,colorPalette):\n",
    "\n",
    "    # Create subject keys and color values\n",
    "    keys = data[measure].unique()\n",
    "    values = sns.color_palette(colorPalette,len(keys))\n",
    "    values = values.as_hex()\n",
    "\n",
    "    # zip dictionary together\n",
    "    colors_dict = dict(zip(keys,values))\n",
    "\n",
    "    return colors_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d12b14-8b9d-49f0-9176-36b20edb379b",
   "metadata": {},
   "source": [
    "### Data analysis, manipulation and statistical computations\n",
    "This next section contains functions for manipulating, analysing, and computing statistics from the data collected using the functions previously described. This includes many different manipulations and computations, including trimming nodes off profilometry data, computing mean dataframes, computing outliers and generating reference datasets, and some simple network adjacency matrix manipulations including binarizing, thresholding, and computing the mean witihin-network functional connectivity. \n",
    "\n",
    "The bulk of these functions are devoted to identifying outliers and generating reference datasets. My approach for this included identifying outliers for each structure and measure across a given project  by computing the mean distribution and identifying those with distances from that point greater than a percentile threshold. I then removed those data points, and recomputed the reference dataset. The references generated have been built-into many of the verification steps following Apps. The resulting dataframes are formatted in both csv and json structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37da556c-41d6-4449-b1f8-debbe7ca7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataframe manipulations\n",
    "## cut nodes for profilometry / timeseries data\n",
    "def cutNodes(data,num_nodes,dataPath,foldername,savename):\n",
    "\n",
    "    # identify inner n nodes based on num_nodes input\n",
    "    total_nodes = len(data['nodeID'].unique())\n",
    "    cut_nodes = int((total_nodes - num_nodes) / 2)\n",
    "\n",
    "    # remove cut_nodes from dataframe\n",
    "    data = data[data['nodeID'].between((cut_nodes)+1,(num_nodes+cut_nodes))]\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'/'+foldername+'-'+savename+'.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "## will compute mean dataframe \n",
    "def computeMeanData(dataPath,data,outname):\n",
    "\n",
    "    # make mean data frame\n",
    "    data_mean =  data.groupby(['subjectID','classID','structureID']).mean().reset_index()\n",
    "    data_mean['nodeID'] = [ 1 for f in range(len(data_mean['nodeID'])) ]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data_mean.to_csv(dataPath+outname+'.csv',index=False)\n",
    "\n",
    "    return data_mean\n",
    "\n",
    "### scripts related to outlier detection and reference dataframe generation\n",
    "## this function will compute distance measures from input data and reference data\n",
    "def computeDistance(data,references_data,measures,metric):\n",
    "\n",
    "    # imports important distance functions\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    from scipy.stats import wasserstein_distance\n",
    "\n",
    "    # if distance metric desired is euclidean distance (i.e. for profiles), computes distance of profile from reference profile. else, just computes difference using emd\n",
    "    if metric == 'euclidean':\n",
    "        dist = data.groupby('subjectID',sort=False).apply(lambda x: euclidean_distances([x[measures].values.tolist(),references_data[measures].values.tolist()])[0][1]).values\n",
    "    else:\n",
    "        dist = data.groupby('subjectID',sort=False).apply(lambda x: wasserstein_distance(x[measures],[references_data[measures].values[0]]))\n",
    "\n",
    "    return dist\n",
    "\n",
    "## this function will compute simple average references for a given input data\n",
    "def computeReferences(x,groupby_measures,index_measure,diff_measures):\n",
    "    \n",
    "    # computes mean and sd of the measures in a dataframe\n",
    "    references_mean = x.groupby(groupby_measures).mean().reset_index(index_measure)\n",
    "    references_sd = x.groupby(groupby_measures).std().reset_index(index_measure)\n",
    "    references_sd[diff_measures] = references_sd[diff_measures] * 2\n",
    "    \n",
    "    return references_mean, references_sd\n",
    "\n",
    "## this function calls computeReferences and computeDistances to create a dataframe of distance measures\n",
    "def createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric):\n",
    "    \n",
    "    # set up output lists that we will append to\n",
    "    dist = []\n",
    "    subj = []\n",
    "    meas = []\n",
    "    struc = []\n",
    "\n",
    "    # loop through appropriate structures\n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        # set data for a given structure\n",
    "        subj_data = data.loc[data['structureID'] == i]\n",
    "        # compute reference for given structure\n",
    "        references_data = computeReferences(subj_data,groupby_measure,groupby_measure,measures)\n",
    "        # loop through measures and compute distance from reference\n",
    "        for m in measures:\n",
    "            if dist_metric == 'euclidean':\n",
    "                dist = np.append(dist,computeDistance(subj_data,references_data[0],m,'euclidean'))\n",
    "            else:\n",
    "                dist = np.append(dist,computeDistance(subj_data,references_data[0],m,'emd'))\n",
    "            \n",
    "            # append data to appropriate lists\n",
    "            subj = np.append(subj,subj_data.subjectID.unique().tolist())\n",
    "            meas = np.append(meas,[ m for f in range(len(subj_data.subjectID.unique().tolist())) ])\n",
    "            struc = np.append(struc,[ i for f in range(len(subj_data.subjectID.unique().tolist())) ])\n",
    "\n",
    "    # create distance dataframe\n",
    "    dist_dataframe = pd.DataFrame()\n",
    "    dist_dataframe['subjectID'] = subj\n",
    "    dist_dataframe['structureID'] = struc\n",
    "    dist_dataframe['measures'] = meas\n",
    "    dist_dataframe['distance'] = dist\n",
    "    \n",
    "    return dist_dataframe\n",
    "\n",
    "## this function is useful for saving reference.jsons for a given structure\n",
    "def outputReferenceJson(ref_data,measures,profile,resample_points,sourceID,data_dir,filename):\n",
    "    \n",
    "    # load module for resampling profile data for generating reference.json\n",
    "    from scipy.signal import resample\n",
    "\n",
    "    # loop through structures in dataframe\n",
    "    for st in ref_data.structureID.unique():\n",
    "        # set up important measures\n",
    "        reference_json = []\n",
    "        tmp = {}\n",
    "        tmp['structurename'] = st\n",
    "        tmp['source'] = sourceID\n",
    "        # loop through measures\n",
    "        for meas in measures:\n",
    "            # grab data\n",
    "            tmp[meas] = {}\n",
    "            if profile:\n",
    "                gb_frame = ref_data.loc[ref_data['structureID'] == st][['nodeID',meas]].dropna().groupby('nodeID')[meas]\n",
    "            else:\n",
    "                gb_frame = ref_data.loc[ref_data['structureID'] == st][[meas]].dropna()[meas]\n",
    "            \n",
    "            # if resample_points is a value, resamples references and computes multiple summary measures.\n",
    "            # else, will just output the entire data. this second option is really only useful for non-profile/series data\n",
    "            data_tmp = []\n",
    "            if resample_points:                \n",
    "                mean_tmp = resample(gb_frame.mean().values.tolist(),resample_points).tolist()\n",
    "                min_tmp = resample(gb_frame.min().values.tolist(),resample_points).tolist()\n",
    "                max_tmp = resample(gb_frame.max().values.tolist(),resample_points).tolist()\n",
    "                sd_tmp = resample(gb_frame.std().values.tolist(),resample_points).tolist()\n",
    "                five_tmp = resample(gb_frame.quantile(q=.05).values.tolist(),resample_points).tolist()\n",
    "                twofive_tmp = resample(gb_frame.quantile(q=.25).values.tolist(),resample_points).tolist()\n",
    "                sevenfive_tmp = resample(gb_frame.quantile(q=.75).values.tolist(),resample_points).tolist()\n",
    "                ninefive_tmp = resample(gb_frame.quantile(q=.95).values.tolist(),resample_points).tolist()\n",
    "                tmp[meas]['mean'] = mean_tmp\n",
    "                tmp[meas]['min'] = min_tmp\n",
    "                tmp[meas]['max'] = max_tmp\n",
    "                tmp[meas]['sd'] = sd_tmp\n",
    "                tmp[meas]['5_percentile'] = five_tmp\n",
    "                tmp[meas]['25_percentile'] = twofive_tmp\n",
    "                tmp[meas]['75_percentile'] = sevenfive_tmp\n",
    "                tmp[meas]['95_percentile'] = ninefive_tmp\n",
    "            else:\n",
    "                data_tmp = gb_frame.values.tolist()\n",
    "                tmp[meas]['data'] = data_tmp\n",
    "        reference_json.append(tmp)\n",
    "\n",
    "        with open(data_dir+'/'+filename+'_'+st+'.json','w') as ref_out_f:\n",
    "            json.dump(reference_json,ref_out_f)\n",
    "    \n",
    "    return reference_json\n",
    "\n",
    "## this function is used to build the reference dataset removing any subjects identified as outliers. the dataframe may or may not be useful\n",
    "def buildReferenceData(data,outliers,profile,data_dir,filename):\n",
    "    \n",
    "    # set up dataframe\n",
    "    reference_data = pd.DataFrame()\n",
    "\n",
    "    # loop through structures and measures and set data\n",
    "    for s in outliers.structureID.unique():\n",
    "        for m in outliers.measures.unique():\n",
    "            if profile:\n",
    "                meas = ['structureID','subjectID','nodeID',m]\n",
    "            else:\n",
    "                meas = ['structureID','subjectID',m]\n",
    "            tmpdata = data[(data[\"structureID\"] == s) & (~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique()))][meas].reset_index(drop=True)\n",
    "            reference_data = pd.concat([reference_data,tmpdata])\n",
    "    # if not profile, will compute average\n",
    "    if not profile:\n",
    "        reference_data = reference_data.groupby(['structureID','subjectID']).mean().reset_index()\n",
    "\n",
    "    reference_data.to_csv(data_dir+'/'+filename+'.csv',index=False)\n",
    "\n",
    "    return reference_data\n",
    "\n",
    "## this function will identify if a given subjects' data is an outlier based on distance from reference and a threshold percentage\n",
    "def computeOutliers(distances,threshold):\n",
    "    \n",
    "    # set up dataframe\n",
    "    outliers = pd.DataFrame()\n",
    "    \n",
    "    # loop through structureas and measures and identify outliers based on thereshold distance\n",
    "    for i in distances.structureID.unique():\n",
    "        for m in distances.measures.unique():\n",
    "            tmpdata = distances.loc[distances['structureID'] == i].loc[distances['measures'] == m]\n",
    "            outliers = pd.concat([outliers,tmpdata[tmpdata['distance'] > np.percentile(tmpdata['distance'],threshold)]])\n",
    "            \n",
    "    return outliers\n",
    "\n",
    "## this function calls computeOutliers, createDistanceDataframe, and buildReferenceData and outputReferenceJson to actually generate the outliers\n",
    "## and final reference datasets\n",
    "def outlierDetection(data,structures,groupby_measure,measures,threshold,dist_metric,build_outliers,profile,resample_points,sourceID,data_dir,filename):\n",
    "    \n",
    "    # set up important lists\n",
    "    outliers_subjects = []\n",
    "    outliers_structures = []\n",
    "    outliers_measures = []\n",
    "    outliers_metrics = []\n",
    "\n",
    "    # compute distances and identify outliers\n",
    "    distances = createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric)\n",
    "    outliers_dataframe = computeOutliers(distances,threshold)\n",
    "    \n",
    "    # if building references, build the reference data. otherwise, output a blank array\n",
    "    if build_outliers:\n",
    "        reference_dataframe = buildReferenceData(data,outliers_dataframe,profile,data_dir,filename)\n",
    "        reference_json = outputReferenceJson(reference_dataframe,measures,profile,resample_points,sourceID,data_dir,filename)\n",
    "    else:\n",
    "        reference_dataframe = []\n",
    "        reference_json = []\n",
    "        \n",
    "    return distances, outliers_dataframe, reference_dataframe, reference_json\n",
    "\n",
    "## this function is useful in identifying subjects who may have had a flipped profile as compared to a reference profile.\n",
    "## calls computeReferences, computeDistance.\n",
    "## logic: if the distance of a subject's tract profile from a reference profile is positive and greater than a threshold percentage, then it's likely\n",
    "## the data has been flipped. needs more work for all use cases. works well for easy examples like uncinate\n",
    "def profileFlipCheck(data,subjects,structures,test_measure,flip_measures,dist_metric,threshold,outPath):\n",
    "    \n",
    "    # set up important lists\n",
    "    flipped_subjects = []\n",
    "    flipped_structures = []\n",
    "    distance = []\n",
    "    flipped_distance = []\n",
    "\n",
    "    # loop through structures\n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        # set data frame for structure, including a copy that has the test_measure flipped\n",
    "        struc_data = data.loc[data['structureID'] == i]\n",
    "        flipped_struc_data = struc_data.copy()\n",
    "        flipped_struc_data[test_measure] = flipped_struc_data.groupby('subjectID',sort=False).apply(lambda x: np.flip(x['fa'])).tolist()\n",
    "\n",
    "        # build reference data\n",
    "        references_data = computeReferences(struc_data,'nodeID','nodeID',flip_measures)\n",
    "\n",
    "        # compute distances for normal data and for flipped. then compute difference\n",
    "        dist = computeDistance(struc_data,references_data[0],test_measure,dist_metric)\n",
    "        dist_flipped = computeDistance(flipped_struc_data,references_data[0],test_measure,dist_metric)\n",
    "        differences = dist - dist_flipped\n",
    "\n",
    "        # identify threshold of distances based on percentile. identify those that have positive differences and are greater than the threshold.\n",
    "        # if so, append information\n",
    "        percentile_threshold = np.percentile(differences,threshold)\n",
    "        for m in range(len(differences)):\n",
    "            if differences[m] > 0 and differences[m] > percentile_threshold:\n",
    "                flipped_subjects = np.append(flipped_subjects,subjects[m])\n",
    "                flipped_structures = np.append(flipped_structures,i)\n",
    "                distance = np.append(distance,dist[m])\n",
    "                flipped_distance = np.append(flipped_distance,dist_flipped[m])\n",
    "\n",
    "    # generate ouput dataframe containing flipped subject data\n",
    "    output_summary = pd.DataFrame()\n",
    "    output_summary['flipped_subjects'] = flipped_subjects\n",
    "    output_summary['flipped_structures'] = flipped_structures\n",
    "    output_summary['distance'] = distance\n",
    "    output_summary['flipped_distance'] = flipped_distance\n",
    "\n",
    "    if outPath:\n",
    "        output_summary.to_csv(outPath+'_flipped_profiles.csv',index=False)\n",
    "        \n",
    "# this function will merge the structural and diffusion data for the reference datasets\n",
    "def mergeStructuralDiffusionJson(data,structuralPath,diffusionPath,outPath):\n",
    "    for i in data.structureID.unique():\n",
    "        print(i)\n",
    "        with open(structuralPath+'_'+i+'.json','r') as structural_f:\n",
    "            structural = json.load(structural_f)\n",
    "\n",
    "        with open(diffusionPath+'_'+i+'.json','r') as diffusion_f:\n",
    "            diffusion = json.load(diffusion_f)\n",
    "\n",
    "        merged = {**structural[0],**diffusion[0]}\n",
    "\n",
    "        with open(outPath+'_'+i+'.json','w') as out_f:\n",
    "            json.dump(merged,out_f)\n",
    "\n",
    "### adjacency-matrix related fuctions for computing network values locally\n",
    "def binarizeMatrices(data):\n",
    "    \n",
    "    # use brain connectivity toolbox to binarize data\n",
    "    bin_data = [ bct.utils.binarize(data[f]) for f in data.keys() ]\n",
    "    \n",
    "    return bin_data\n",
    "\n",
    "def thresholdMatrices(data,bin_data,thresholdPercentageSubjects):\n",
    "    \n",
    "    # compute sum of matrices\n",
    "    sum_mat = np.zeros(np.shape(bin_data)[1:])\n",
    "    \n",
    "    for i in bin_data:\n",
    "        sum_mat = sum_mat + i\n",
    "        \n",
    "    # compute threshold value\n",
    "    thrs = thresholdPercentageSubjects*len(bin_data)\n",
    "    \n",
    "    # loop through data and make those nodes that dont meet threshold 0\n",
    "    for i in data.keys():\n",
    "        data[i][sum_mat < thrs] = 0\n",
    "        \n",
    "    return data\n",
    "\n",
    "def computeMeanNetworkConnectivity(data,networks,indices,out_path):\n",
    "\n",
    "    mean_data = []\n",
    "    subs = []\n",
    "    nets = []\n",
    "\n",
    "    for l in data.keys():\n",
    "        for n in networks:\n",
    "            tmpdata = []\n",
    "            subs = np.append(subs,l)\n",
    "            nets = np.append(nets,n)\n",
    "            for i in indices[n]:\n",
    "                for j in indices[n]:\n",
    "                    if i != j:\n",
    "                        tmpdata = np.append(tmpdata,data[l][int(i)][int(j)])\n",
    "            mean_data = np.append(mean_data,np.mean(tmpdata))\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "    out_df['subjectID'] = [ f.split('_sess1')[0] for f in subs ]\n",
    "    out_df['FC'] = mean_data\n",
    "    out_df = pd.merge(out_df,subjects_data,on='subjectID')\n",
    "    out_df['structureID'] = nets\n",
    "\n",
    "    if out_path:\n",
    "        out_df.to_csv(out_path,index=False)\n",
    "        \n",
    "    return out_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb7642d-4ac3-441a-9916-6b131ed7b6d6",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "The final set of functions deals with generating many useful visualizations and plots of the data, including scatterplots, violin plots, and tract profile plots. Within this are many useful wrapper functions that organize the data into useful structures for plotting. These scripts are very 'baked in', in that they are in a style that I like. However, many of these visualizations can be changed with simple alterations to the code. Many of the plots use packages like seaborn or matplotlib to make the visualizations, which both have extensive documentation to guide and help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25554f3c-a02b-491b-a0ae-790acb080ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### scatter plot related scripts\n",
    "def plotProfiles(structures,stat,diffusion_measures,summary_method,error_method,dir_out,img_name):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os,sys\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "\n",
    "    # loop through all structures\n",
    "    for t in structures:\n",
    "        print(t)\n",
    "        # loop through all measures\n",
    "        for dm in diffusion_measures:\n",
    "            print(dm)\n",
    "\n",
    "            imgname=img_name+\"_\"+t+\"_\"+dm\n",
    "\n",
    "            # generate figures\n",
    "            fig = plt.figure(figsize=(15,15))\n",
    "            fig.patch.set_visible(False)\n",
    "            p = plt.subplot()\n",
    "\n",
    "            # set title and catch array for legend handle\n",
    "            plt.title(\"%s Profiles %s: %s\" %(summary_method,t,dm),fontsize=20)\n",
    "\n",
    "            # loop through groups and plot profile data\n",
    "            for g in range(len(stat.classID.unique())):\n",
    "                # x is nodes\n",
    "                x = stat['nodeID'].unique()\n",
    "\n",
    "                # y is summary (mean, median, max, main) profile data\n",
    "                if summary_method == 'mean':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).mean()[dm][t]\n",
    "                elif summary_method == 'median':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).median()[dm][t]\n",
    "                elif summary_method == 'max':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).max()[dm][t]\n",
    "                elif summary_method == 'min':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).min()[dm][t]\n",
    "\n",
    "                # error bar is either: standard error of mean (sem), standard deviation (std)\n",
    "                if error_method == 'sem':\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t] / np.sqrt(len(stat[stat['classID'] == stat.classID.unique()[g]]['subjectID'].unique()))\n",
    "                else:\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t]\n",
    "\n",
    "                # plot summary\n",
    "                plt.plot(x,y,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],linewidth=5,label=stat.classID.unique()[g])\n",
    "\n",
    "                # plot shaded error\n",
    "                plt.fill_between(x,y-err,y+err,alpha=0.2,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],label='1 %s %s' %(error_method,stat.classID.unique()[g]))\n",
    "\n",
    "            # set up labels and ticks\n",
    "            plt.xlabel('Location',fontsize=18)\n",
    "            plt.ylabel(dm,fontsize=18)\n",
    "            plt.xticks([x[0],x[-1]],['Begin','End'],fontsize=16)\n",
    "            plt.legend(fontsize=16)\n",
    "            y_lim = plt.ylim()\n",
    "            plt.yticks([np.round(y_lim[0],2),np.mean(y_lim),np.round(y_lim[1],2)],fontsize=16)\n",
    "\n",
    "            # remove top and right spines from plot\n",
    "            p.axes.spines[\"top\"].set_visible(False)\n",
    "            p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "            # save image or show image\n",
    "            saveOrShowImg(dir_out,dm,dm,imgname)\n",
    "\n",
    "\n",
    "# groups data by input measure and computes mean for each value in that column. x_stat is a pd dataframe, with each row being a single value, and each column being a different ID value or measure\n",
    "def averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X = x_stat.groupby(measure).mean()[x_measure].tolist()\n",
    "    Y = y_stat.groupby(measure).mean()[y_measure].tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# groups data by input measure and creates an array by appending data into x and y arrays. x_stat and y_stat are pd dataframes, with each row being a single value, and each column being a different ID value or measure\n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X,Y = [np.array([]),np.array([])]\n",
    "    for i in range(len(x_stat[measure].unique())):\n",
    "        x = x_stat[x_stat[measure] == x_stat[measure].unique()[i]][x_measure]\n",
    "        y = y_stat[y_stat[measure] == y_stat[measure].unique()[i]][y_measure]\n",
    "\n",
    "        if np.isnan(x).any() or np.isnan(y).any():\n",
    "            print(\"skipping %s due to nan\" %x_stat[measure].unique()[i])\n",
    "        else:\n",
    "            # checks to make sure the same data\n",
    "            if len(x) == len(y):\n",
    "                X = np.append(X,x)\n",
    "                Y = np.append(Y,y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels networks. x_stat and y_stat should be S x M, where S is the number of subjects and M is the adjacency matrix for that subject\n",
    "def ravelNetwork(x_stat,y_stat):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    X = np.ravel(x_stat).tolist()\n",
    "    Y = np.ravel(y_stat).tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels nonnetwork data. x_stat and y_stat should be pd dataframes. x_measure and y_measure are the measure to unrvavel. \n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def ravelNonNetwork(x_stat,y_stat,x_measure,y_measure):\n",
    "\n",
    "    X = x_stat[x_measure].to_list()\n",
    "    Y = y_stat[y_measure].to_list()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# wrapper function to call either of the above scripts based on user input\n",
    "def setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,isnetwork,measure):\n",
    "\n",
    "    x_stat = x_data\n",
    "    y_stat = y_data\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        X,Y = averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'append':\n",
    "        X,Y = appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'ravel':\n",
    "        if isnetwork == True:\n",
    "            X,Y = ravelNetwork(x_stat,y_stat)\n",
    "        else:\n",
    "            X,Y = ravelNonNetwork(x_stat,y_stat,x_measure,y_measure)\n",
    "\n",
    "    return x_stat,y_stat,X,Y\n",
    "\n",
    "# function to shuffle data and colors\n",
    "def shuffleDataAlg(X,Y,hues):\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    X,Y,hues = shuffle(X,Y,hues)\n",
    "\n",
    "    return X,Y,hues\n",
    "\n",
    "# simple display or figure save function\n",
    "def saveOrShowImg(dir_out,x_measure,y_measure,img_name):\n",
    "    import os,sys \n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    " \n",
    "        # save or show plot\n",
    "        if dir_out:\n",
    "            if not os.path.exists(dir_out):\n",
    "                os.mkdir(dir_out)\n",
    "\n",
    "            if x_measure == y_measure:\n",
    "                img_name_eps = img_name+'_'+x_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'.svg'\n",
    "            else:\n",
    "                img_name_eps = img_name+'_'+x_measure+'_vs_'+y_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'_vs_'+y_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'_vs_'+y_measure+'.svg'\n",
    "\n",
    "            plt.savefig(os.path.join(dir_out, img_name_eps),transparent=True)\n",
    "            plt.savefig(os.path.join(dir_out, img_name_png))     \n",
    "    #         plt.savefig(os.path.join(dir_out, img_name_svg))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "    \n",
    "# uses seaborn's relplot function to plot data for each unique value in a column of a pandas dataframe (ex. subjects, structureID). useful for supplementary figures or sanity checking or preliminary results\n",
    "# column measure is the measure within which each unique value will have its own plot. hue_measure is the column to use for coloring the data. column_wrap is how many panels you want per row\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def relplotScatter(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,column_wrap,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data: CANNOT BE AVERAGE\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,'ravel',False,hue_measure)\n",
    "\n",
    "    p = sns.relplot(x=X,y=Y,col=x_stat[column_measure],hue=x_stat[hue_measure],kind=\"scatter\",s=100,col_wrap=column_wrap)\n",
    "\n",
    "    # setting counter. looping through axes to add important info and regression lines\n",
    "    i = 0\n",
    "    for ax in p.axes.flat:\n",
    "        x_lim,y_lim = [ax.get_xlim(),ax.get_ylim()]\n",
    "\n",
    "        if trendline == 'equality':\n",
    "            ax.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "        elif trendline == 'linreg':\n",
    "            m,b = np.polyfit(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'],1)\n",
    "            ax.plot(ax.get_xticks(),m*ax.get_xticks() + b)\n",
    "            plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        ax.set_xlim(x_lim)\n",
    "        ax.set_ylim(y_lim)\n",
    "        ax.set_xlabel(x_measure)\n",
    "        ax.set_ylabel(y_measure)\n",
    "\n",
    "        # compute correlation for each subject and add to plots\n",
    "        corr = np.corrcoef(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'])[1][0]\n",
    "        plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # compute rmse for each subject and add to plots\n",
    "        rmse = np.sqrt(mean_squared_error(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y']))\n",
    "        plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # update counter\n",
    "        i = i+1\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure]. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def singleplotScatter(colors_dict,x_data,y_data,x_measure,y_measure,logX,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,colorDistance,perfectOrSlope,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "    \n",
    "    if colorDistance:\n",
    "        category = colorDistanceScatter(X,Y,perfectOrSlope)\n",
    "    else:\n",
    "        colors = sns.color_palette('colorblind',len(x_stat[hue_measure]))\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    if logX == True:\n",
    "        X = np.log10(X)\n",
    "\n",
    "    if colors_dict:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    elif colorDistance:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=category,hue_order=['two-sd','one-sd','lt-one-sd'],palette=['red','green','blue'],s=100)\n",
    "    else:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100)\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    if x_measure == y_measure:\n",
    "        p.axes.axis('square')\n",
    "        y_ticks = p.axes.get_yticks()\n",
    "        p.axes.set_xticks(y_ticks)\n",
    "        p.axes.set_yticks(p.axes.get_xticks())\n",
    "        p.axes.set_ylim(p.axes.get_xlim())\n",
    "        p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "        ax = plt.gca()\n",
    "        ax.get_legend().remove()\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(X,Y,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "        ax = plt.gca()\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    elif trendline == 'groupreg':\n",
    "        for g in range(len(groups)):\n",
    "            if stat_name == 'volume':\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            else:\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(X,Y)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(X,Y))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "    \n",
    "def colorDistanceScatter(x,y,perfectOrSlope):\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    import math\n",
    "\n",
    "    ### this process creates a list of sd categories by rotating the x,y distribution\n",
    "    ### by 45 degrees (to make the slope essentially zero) and computing the standard\n",
    "    ### deviation along the y-axis. then, it identifies whether the y-data falls either\n",
    "    ### within 1 sd, within 1-2 sd, and greater than 2 sds\n",
    "    \n",
    "    # if users want to compute distribution around perfect 45 deg equality line or around\n",
    "    # the actual data slope\n",
    "    if perfectOrSlope == True:\n",
    "        m = 1\n",
    "    else:\n",
    "        m,b = np.polyfit(x,y,1)\n",
    "    \n",
    "    # compute theta as the clockwise atan rotation along m\n",
    "    theta = -math.atan(m)\n",
    "    \n",
    "    # generate rotation matrix\n",
    "    r = np.array([[np.cos(theta),-np.sin(theta)],[np.sin(theta),np.cos(theta)]])\n",
    "    \n",
    "    # compute rotation\n",
    "    [x_dif,y_dif] = r.dot(np.array([x,y]))\n",
    "\n",
    "    # set output variable (category)\n",
    "    category = []\n",
    "    \n",
    "    # compute standard deviation thresholds for difference values\n",
    "    one_sd = np.std(np.abs(y_dif))\n",
    "    two_sd = one_sd * 2\n",
    "\n",
    "    # loop through each data point and determine category (one sd: within 1 and 2 sds, two-sd: greater or equal to 2 sds, lt-one-sd: within one sd)\n",
    "    category = [ 'one-sd' if one_sd <= f < two_sd else 'two-sd' if f >= two_sd else 'lt-one-sd' for f in np.abs(y_dif) ]\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2638e08-2c42-458d-a8a5-09f16f51b328",
   "metadata": {},
   "source": [
    "## Generating subject demographic dataframe\n",
    "Now that we've loaded all the functions needed, but before we get started collecting and analyzing our data, we first need to generate a subjects demographic database. This can include information stored in the participants.json file stored in the Project's Detail tab, but can also be user-generated. This is so we can then append that relevant information to each of the data structures we collect, compile, and generate for analysis. This makes it easier to build models and to perform statistical analyses with co-variates.\n",
    "\n",
    "In the case of Hayashi & Caron et al (in prep), since I was using data from multiple Big Data repositories including the Human Connectome Project, the Cambridge Centre for Ageing and Neuroscience (Cam-CAN), and the Pediatric Imaging, Neurocognition, and Genetics (PING) Dataset  I uploaded the important subject identification and demographic data directly from each project's respective repository. I scrubbed the data of most of the relevant demographic information, as I was mostly only concerned with the participant's subject ID, age, and gender.\n",
    "\n",
    "In order to ensure participant anonymity, I've only included the subjectID in the example subjects_data.csv dataframe.\n",
    "\n",
    "Within this dataframe, I've also added a classID column corresponding to the project identification name (i.e. hcp, camcan, ping) and a colors column corresponding to the color I assigned to each project (hcp: green, camcan: orange, ping: purple).\n",
    "\n",
    "After this, I then created a color dictionary with a unique color for each participant in the project. This can make outlier detection a bit easier.\n",
    "\n",
    "In this example notebook, I will be using the HCP dataset!\n",
    "\n",
    "Finally, within this section we will also set up our data and image directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab4650c-11fa-4cb6-8340-794d2527f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up variables\n",
      "setting up variables complete\n",
      "grabbing demographic data\n",
      "grabbing demographic data complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16191/3687513580.py:27: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  subjects_data['subjectID'] = [ str(int(np.float(f))) for f in subjects_data.subjectID ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectID</th>\n",
       "      <th>classID</th>\n",
       "      <th>colors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100004</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100206</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100307</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100408</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100610</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>992774</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>993675</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>994273</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>995174</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>996782</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1206 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subjectID classID colors\n",
       "0       100004     hcp   blue\n",
       "1       100206     hcp   blue\n",
       "2       100307     hcp   blue\n",
       "3       100408     hcp   blue\n",
       "4       100610     hcp   blue\n",
       "...        ...     ...    ...\n",
       "1201    992774     hcp   blue\n",
       "1202    993675     hcp   blue\n",
       "1203    994273     hcp   blue\n",
       "1204    995174     hcp   blue\n",
       "1205    996782     hcp   blue\n",
       "\n",
       "[1206 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### setting up variables and adding paths\n",
    "print(\"setting up variables\")\n",
    "topPath = \"./\"\n",
    "os.chdir(topPath)\n",
    "data_dir = topPath+'/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "img_dir = topPath+'/img/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "# set up project identification (groups) and the project color (green)\n",
    "groups = ['hcp']\n",
    "colors_array = ['green']\n",
    "print(\"setting up variables complete\")\n",
    "\n",
    "### grabbing subjects demographic data\n",
    "print(\"grabbing demographic data\")\n",
    "if os.path.isfile(data_dir+'/subjects_data.csv'):\n",
    "    subjects_data = pd.read_csv(data_dir+'/subjects_data.csv')\n",
    "else:\n",
    "    subjects_data = pd.read_json('input/participants.json')\n",
    "print(\"grabbing demographic data complete\")\n",
    "\n",
    "# hcp stored their subject IDs as integers instead of strings. changing that here\n",
    "if subjects_data['subjectID'].dtype == 'int':\n",
    "    subjects_data['subjectID'] = [ str(int(np.float(f))) for f in subjects_data.subjectID ]\n",
    "\n",
    "# create column for classID and colors\n",
    "subjects_data['classID'] = [ 'hcp' for f in subjects_data['subjectID'] ]\n",
    "subjects_data['colors'] = [ 'blue' for f in subjects_data['subjectID'] ]\n",
    "\n",
    "### generate colors dictionary with a unique color for each subject\n",
    "colors = {}\n",
    "subjects = {}\n",
    "\n",
    "# loop through groups and identify subjects and set color schema for each group\n",
    "for g in range(len(groups)):\n",
    "    # set subjects array\n",
    "    subjects[groups[g]] =  subjects_data['subjectID']\n",
    "    subjects[groups[g]].sort_values()\n",
    "    \n",
    "    # set colors array\n",
    "    colors_name = colors_array[g]\n",
    "    colors[groups[g]] = colors_array[g]\n",
    "\n",
    "# create subjects color dictionary\n",
    "colors_dict = createColorDictionary(subjects_data,'subjectID','colorblind')\n",
    "\n",
    "## here's what the subjects_data dataframe looks like!\n",
    "subjects_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55419d04-7e74-481f-a072-5616cda1c725",
   "metadata": {},
   "source": [
    "# Collecting functional network adjacency matrices\n",
    "The next step of the analysis pipeline for me is to collect and analyze the functional connectivity matrices generated from functional MRI. To do this, I created matrices of the functional connectivity between parcels in the Yeo17 atlas for each participant. I then computed the 'within-network' average functional connectivity for each resting-state network in the Yeo atlas within each participant. This followed procedures outlined in This data was then used in Betzel et al (2014)[https://pubmed.ncbi.nlm.nih.gov/25109530/]. This data was used in Figure XX to identify the relationship between average within-network functional connectivity and participant age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "252b339a-2747-4b0a-91dd-c12039d9b269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectID</th>\n",
       "      <th>FC</th>\n",
       "      <th>classID</th>\n",
       "      <th>colors</th>\n",
       "      <th>structureID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100307</td>\n",
       "      <td>0.256095</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>VisCent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100307</td>\n",
       "      <td>0.692158</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>VisPeri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100307</td>\n",
       "      <td>0.940064</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>SomMotA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100307</td>\n",
       "      <td>0.535629</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>SomMotB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100307</td>\n",
       "      <td>0.581255</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>DorsAttnA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>329844</td>\n",
       "      <td>0.480598</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>ContB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9086</th>\n",
       "      <td>329844</td>\n",
       "      <td>0.517073</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>ContC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>329844</td>\n",
       "      <td>0.512753</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>DefaultA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>329844</td>\n",
       "      <td>0.361030</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>DefaultB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>329844</td>\n",
       "      <td>0.494148</td>\n",
       "      <td>hcp</td>\n",
       "      <td>blue</td>\n",
       "      <td>DefaultC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9090 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subjectID        FC classID colors structureID\n",
       "0       100307  0.256095     hcp   blue     VisCent\n",
       "1       100307  0.692158     hcp   blue     VisPeri\n",
       "2       100307  0.940064     hcp   blue     SomMotA\n",
       "3       100307  0.535629     hcp   blue     SomMotB\n",
       "4       100307  0.581255     hcp   blue   DorsAttnA\n",
       "...        ...       ...     ...    ...         ...\n",
       "9085    329844  0.480598     hcp   blue       ContB\n",
       "9086    329844  0.517073     hcp   blue       ContC\n",
       "9087    329844  0.512753     hcp   blue    DefaultA\n",
       "9088    329844  0.361030     hcp   blue    DefaultB\n",
       "9089    329844  0.494148     hcp   blue    DefaultC\n",
       "\n",
       "[9090 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### fmri yeo networks - max node degree vs age replication\n",
    "if os.path.isfile(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency.npy'):\n",
    "    fmri_yeo_networks_adjacency = np.load(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency.npy',allow_pickle=True).item()\n",
    "else:\n",
    "    fmri_yeo_networks_adjacency = collectData('generic/network',[\"time series\"],['yeo-mni','final'],'network.json.gz',data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency',True)\n",
    "\n",
    "# grab label indices\n",
    "networks = ['VisCent','VisPeri','SomMotA','SomMotB','DorsAttnA','DorsAttnB','SalVentAttnA','SalVentAttnB','Limbic','ContA','ContB','ContC','DefaultA','DefaultB','DefaultC']\n",
    "with open(data_dir+'/label.json','r') as label_f:\n",
    "    labels = json.load(label_f)\n",
    "\n",
    "indices = {}\n",
    "for n in range(len(networks)):\n",
    "    indices[networks[n]] = []\n",
    "    for i in range(len(labels)):\n",
    "        netname = labels[i]['name']\n",
    "        if networks[n] in netname:\n",
    "                indices[networks[n]] = np.append(indices[networks[n]],int(i))\n",
    "\n",
    "# create average matrix\n",
    "if os.path.isfile(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency_mean.npy'):\n",
    "    fmri_yeo_networks_adjacency_mean = np.load(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency_mean.npy')\n",
    "else:\n",
    "    # begin process of creating mean network\n",
    "    fmri_yeo_networks_adjacency_mean = fmri_yeo_networks_adjacency[list(fmri_yeo_networks_adjacency.keys())[0]]\n",
    "\n",
    "    # loop through all subjects and add the node values\n",
    "    for i in fmri_yeo_networks_adjacency.keys():\n",
    "        fmri_yeo_networks_adjacency_mean = fmri_yeo_networks_adjacency_mean + fmri_yeo_networks_adjacency[i]\n",
    "    \n",
    "    # compute mean\n",
    "    fmri_yeo_networks_adjacency_mean = fmri_yeo_networks_adjacency_mean / len(fmri_yeo_networks_adjacency.keys())\n",
    "    \n",
    "    # save dataframe\n",
    "    np.save(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency_mean.npy',fmri_yeo_networks_adjacency_mean)\n",
    "    \n",
    "# compute average functional connectivity (within network) for each network in yeo17\n",
    "fmri_yeo_networks_adjacency_mean_fc = computeMeanNetworkConnectivity(fmri_yeo_networks_adjacency,networks,indices,\"\")\n",
    "\n",
    "fmri_yeo_networks_adjacency_mean_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17fe210-c5a3-4b9c-8106-62079a2107be",
   "metadata": {},
   "source": [
    "## Removing outliers and creating reference datasets\n",
    "Once all of the appropriate data was collected, collated, and cleaned, I was now able to generate useful 'reference datasets' that can be used to identify potentially problematic participant data. I adopted a similar methodology as in the Pestilli et al (2014) LiFE paper which created \"reference datasets\", i.e. dataset of values of high-quality and high-diverse data, of properties derived from the LiFE model. The authors of this publication have released this data in the backend of the LiFE app here on brainlife.io, allowing users to directly compare their newly generated results with previously currated results.\n",
    "\n",
    "For this project, I took a similar approach across three large and diverse datasets: PING, HCP, and CAMCAN. For each data modality and structure, I computed the average value across many different models and measures including structural statistics derived from Freesurfer, diffusion microsturctural statistics derived from the DTI and NODDI models, and network measures such as max node degree from structural connectivity measures and average functional connectivity derived from fMRI. I then identified those data points that were outside the top and bottom 5% of the distribution, removed them, and recomputed the distribution. This newly cleaned distribution is the 'reference dataset' for that datatype, measure, and dataset.\n",
    "\n",
    "Following iterating through many datatypes, measures, structures, and projects, I released the final reference datasets on github and we have incorporated the data into the validators for the apps that generate those particular datatypes! This way users can directly compare their newly generated data with those I processed, analyzed, and curated!!\n",
    "\n",
    "This code can also be used to remove outliers from individual study designs, as the methods for identifying outliers and removing them from the data involves simple distribution sampling and percentile thresholding.\n",
    "\n",
    "For this notebook, I will demonstrate how to create reference dataframes from the functional parcels found in the Yeo17 atlas across all participants in the HCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a97fc790-83f6-450b-9b3e-1eb9cd5fe16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating references for within-network average functional connectivity data\n",
      "VisCent\n",
      "VisPeri\n",
      "SomMotA\n",
      "SomMotB\n",
      "DorsAttnA\n",
      "DorsAttnB\n",
      "SalVentAttnA\n",
      "SalVentAttnB\n",
      "Limbic\n",
      "ContA\n",
      "ContB\n",
      "ContC\n",
      "DefaultA\n",
      "DefaultB\n",
      "DefaultC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structureID</th>\n",
       "      <th>subjectID</th>\n",
       "      <th>FC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ContA</td>\n",
       "      <td>100307</td>\n",
       "      <td>0.257720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ContA</td>\n",
       "      <td>101915</td>\n",
       "      <td>0.262575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ContA</td>\n",
       "      <td>102008</td>\n",
       "      <td>0.250266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ContA</td>\n",
       "      <td>102109</td>\n",
       "      <td>0.272175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ContA</td>\n",
       "      <td>102311</td>\n",
       "      <td>0.277649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ContA</td>\n",
       "      <td>102513</td>\n",
       "      <td>0.244219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ContA</td>\n",
       "      <td>102614</td>\n",
       "      <td>0.349925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ContA</td>\n",
       "      <td>102715</td>\n",
       "      <td>0.323933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ContA</td>\n",
       "      <td>102816</td>\n",
       "      <td>0.333471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ContA</td>\n",
       "      <td>103010</td>\n",
       "      <td>0.337580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  structureID subjectID        FC\n",
       "0       ContA    100307  0.257720\n",
       "1       ContA    101915  0.262575\n",
       "2       ContA    102008  0.250266\n",
       "3       ContA    102109  0.272175\n",
       "4       ContA    102311  0.277649\n",
       "5       ContA    102513  0.244219\n",
       "6       ContA    102614  0.349925\n",
       "7       ContA    102715  0.323933\n",
       "8       ContA    102816  0.333471\n",
       "9       ContA    103010  0.337580"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### creating reference datasets\n",
    "print(\"creating references for within-network average functional connectivity data\")\n",
    "\n",
    "# if empty, create references directory\n",
    "if not os.path.isdir(data_dir+'/references'):\n",
    "    os.mkdir(data_dir+'/references')\n",
    "\n",
    "# set threshold\n",
    "thrs = 95\n",
    "\n",
    "if os.path.isfile(data_dir+'/references/fmri_within_network_fc_'+groups[0]+'_'+str(thrs)+'.csv'):\n",
    "    fmri_within_network_fc_reference_dataframe = pd.read_csv(data_dir+'/references/fmri_within_network_fc_'+groups[0]+'_'+str(thrs)+'.csv')\n",
    "else:\n",
    "    # compute reference dataset of fmri within network FC\n",
    "    fmri_within_network_fc_measures = ['FC']\n",
    "    fmri_within_network_fc_distances, fmri_within_network_fc_outliers, fmri_within_network_fc_reference_dataframe, fmri_within_network_fc_reference_json = outlierDetection(fmri_yeo_networks_adjacency_mean_fc,fmri_yeo_networks_adjacency_mean_fc.structureID.unique(),'structureID',['FC'],95,'emd',True,False,'',groups[0],data_dir+'/references','fmri_within_network_fc_'+groups[0]+'_'+str(thrs))\n",
    "    fmri_within_network_fc_reference_dataframe.loc[fmri_within_network_fc_reference_dataframe['structureID'] == 'DefaultA']\n",
    "\n",
    "# show datatframe\n",
    "fmri_within_network_fc_reference_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a854e5-41e9-4379-87ae-a05ca21b216c",
   "metadata": {},
   "source": [
    "## Generating visualizations and plots\n",
    "These are visualizations describing the reference dataframe generation. For the sake of this notebook, I will just show the figures for the within-network' functional connectivity reference dataframe results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c1278c1-a87f-4323-bc40-84c30182ce23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### generate figures documenting outlier detection capabilities\n",
    "# fmri networks - average degree: DMN\n",
    "# grab default mode a network nodes and average\n",
    "if os.path.isfile(data_dir+'/'+groups[0]+'-within-network-default-a-preclean-fc.csv'):\n",
    "    default_a_preclean_df = pd.read_csv(data_dir+'/'+groups[0]+'-within-network-default-a-preclean-fc.csv')\n",
    "else:\n",
    "    default_a_preclean_df = fmri_yeo_networks_adjacency_mean_fc.loc[fmri_yeo_networks_adjacency_mean_fc['structureID'] == 'DefaultA']\n",
    "    default_a_preclean_df.to_csv(data_dir+'/'+groups[0]+'-within-network-default-a-preclean-fc.csv',index=False)\n",
    "\n",
    "# generate distribution of volume pre outliers\n",
    "sns.violinplot(x='FC',data=default_a_preclean_df,scale='count',inner='points')\n",
    "plt.savefig('./img/default_a_fc_pre_outliers_example.png')\n",
    "plt.savefig('./img/default_a_fc_pre_outliers_example.eps')\n",
    "# plt.close()\n",
    "\n",
    "# generate outliers and reference plot\n",
    "default_a_reference_dataframe = fmri_within_network_fc_reference_dataframe.loc[fmri_within_network_fc_reference_dataframe['structureID'] == 'DefaultA']\n",
    "# generate distribution of node degree with outliers\n",
    "outliers_subs = default_a_outliers['subjectID'].unique()\n",
    "default_a_reference = default_a_preclean_df.loc[~default_a_preclean_df['subjectID'].isin(outliers_subs)]\n",
    "default_a_outliers = default_a_preclean_df.loc[default_a_preclean_df['subjectID'].isin(outliers_subs)]\n",
    "sns.violinplot(x='FC',data=default_a_reference,scale='count',inner='points')\n",
    "sns.swarmplot(x='FC',data=default_a_outliers,color='red')\n",
    "plt.savefig('./img/default_a_fc_outliers_example.png')\n",
    "plt.savefig('./img/default_a_fc_outliers_example.eps')\n",
    "# plt.close()\n",
    "\n",
    "# generate reference\n",
    "sns.violinplot(x='FC',data=default_a_reference,scale='count',inner='points')\n",
    "plt.savefig('./img/default_a_fc_reference_example.png')\n",
    "plt.savefig('./img/default_a_fc_reference_example.eps')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f389fa3-2e20-4545-a1cf-f796e24e2321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
